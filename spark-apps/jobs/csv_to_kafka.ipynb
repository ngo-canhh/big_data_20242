{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a36bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 23:26:43,531 - INFO - Attempting to connect to Kafka broker at localhost:9092 to check/create topic...\n",
      "2025-05-23 23:26:43,534 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "2025-05-23 23:26:43,565 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=bootstrap-0 host=localhost:9092 <checking_api_versions_recv> [IPv6 ('::1', 9092, 0, 0)]>: Broker version identified as 2.6\n",
      "2025-05-23 23:26:43,566 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "2025-05-23 23:26:43,603 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "2025-05-23 23:26:43,605 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=1 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "2025-05-23 23:26:43,606 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "2025-05-23 23:26:43,615 - INFO - Successfully connected to Kafka for topic management.\n",
      "2025-05-23 23:26:43,618 - INFO - Topic player-data-topic already exists.\n",
      "2025-05-23 23:26:43,619 - INFO - <BrokerConnection client_id=kafka-python-2.1.5, node_id=1 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "2025-05-23 23:26:43,621 - INFO - Connecting to Kafka broker at localhost:9092...\n",
      "2025-05-23 23:26:43,625 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "2025-05-23 23:26:43,646 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=bootstrap-0 host=localhost:9092 <checking_api_versions_recv> [IPv6 ('::1', 9092, 0, 0)]>: Broker version identified as 2.6\n",
      "2025-05-23 23:26:43,646 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "2025-05-23 23:26:43,647 - ERROR - CRITICAL: Error connecting to Kafka producer: 'KafkaProducer' object has no attribute 'list_topics'\n",
      "2025-05-23 23:26:43,649 - INFO - Attempting to read data from file: ../data/Cleaned_Data.csv\n",
      "2025-05-23 23:26:43,677 - INFO - Successfully read 1573 rows from ../data/Cleaned_Data.csv.\n",
      "2025-05-23 23:26:43,764 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "2025-05-23 23:26:43,767 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=1 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "2025-05-23 23:26:43,768 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "2025-05-23 23:26:43,811 - INFO - Processed 100 rows so far. Prepared 100 messages for sending.\n",
      "2025-05-23 23:26:43,852 - INFO - Processed 200 rows so far. Prepared 200 messages for sending.\n",
      "2025-05-23 23:26:43,889 - INFO - Processed 300 rows so far. Prepared 300 messages for sending.\n",
      "2025-05-23 23:26:43,919 - INFO - Processed 400 rows so far. Prepared 400 messages for sending.\n",
      "2025-05-23 23:26:43,951 - INFO - Processed 500 rows so far. Prepared 500 messages for sending.\n",
      "2025-05-23 23:26:43,982 - INFO - Processed 600 rows so far. Prepared 600 messages for sending.\n",
      "2025-05-23 23:26:44,012 - INFO - Processed 700 rows so far. Prepared 700 messages for sending.\n",
      "2025-05-23 23:26:44,046 - INFO - Processed 800 rows so far. Prepared 800 messages for sending.\n",
      "2025-05-23 23:26:44,077 - INFO - Processed 900 rows so far. Prepared 900 messages for sending.\n",
      "2025-05-23 23:26:44,105 - INFO - Processed 1000 rows so far. Prepared 1000 messages for sending.\n",
      "2025-05-23 23:26:44,138 - INFO - Processed 1100 rows so far. Prepared 1100 messages for sending.\n",
      "2025-05-23 23:26:44,172 - INFO - Processed 1200 rows so far. Prepared 1200 messages for sending.\n",
      "2025-05-23 23:26:44,215 - INFO - Processed 1300 rows so far. Prepared 1300 messages for sending.\n",
      "2025-05-23 23:26:44,256 - INFO - Processed 1400 rows so far. Prepared 1400 messages for sending.\n",
      "2025-05-23 23:26:44,294 - INFO - Processed 1500 rows so far. Prepared 1500 messages for sending.\n",
      "2025-05-23 23:26:44,331 - INFO - ------------------------------\n",
      "2025-05-23 23:26:44,340 - INFO - CSV processing finished.\n",
      "2025-05-23 23:26:44,346 - INFO - Total rows processed: 1573\n",
      "2025-05-23 23:26:44,369 - INFO - Rows skipped or failed during preparation: 0\n",
      "2025-05-23 23:26:44,378 - INFO - Attempted to send 1573 messages to Kafka.\n",
      "2025-05-23 23:26:44,383 - INFO - Flushing Kafka producer (waiting for pending messages)...\n",
      "2025-05-23 23:26:44,602 - INFO - Kafka producer flushed.\n",
      "2025-05-23 23:26:44,602 - INFO - Closing Kafka producer.\n",
      "2025-05-23 23:26:44,603 - INFO - <BrokerConnection client_id=kafka-python-producer-2, node_id=1 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "2025-05-23 23:26:44,605 - INFO - Script finished.\n"
     ]
    }
   ],
   "source": [
    "# --- START OF FILE csv_to_kafka.py ---\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import signal\n",
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# --- Cấu hình Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Cấu hình ---\n",
    "KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'localhost:9092')\n",
    "KAFKA_TOPIC = os.getenv('KAFKA_TOPIC', 'player-data-topic') # Đặt tên topic mới cho dữ liệu cầu thủ\n",
    "CSV_FILE_PATH = os.getenv('CSV_FILE_PATH', '../data/Cleaned_Data.csv') # Đường dẫn tới file CSV\n",
    "\n",
    "# --- Khởi tạo Kafka Producer ---\n",
    "producer = None\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    logger.info(\"Received interrupt signal. Cleaning up...\")\n",
    "    if producer:\n",
    "        producer.flush(timeout=60)\n",
    "        producer.close()\n",
    "    logger.info(\"Cleanup completed. Exiting.\")\n",
    "    exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "# --- Kiểm tra và tạo Kafka Topic ---\n",
    "def ensure_kafka_topic():\n",
    "    try:\n",
    "        # Wait for Kafka to be available if running in a distributed environment\n",
    "        logger.info(f\"Attempting to connect to Kafka broker at {KAFKA_BROKER_URL} to check/create topic...\")\n",
    "        admin_client = None\n",
    "        max_retries = 10\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                admin_client = KafkaAdminClient(bootstrap_servers=[KAFKA_BROKER_URL], request_timeout_ms=5000)\n",
    "                admin_client.list_topics() # Check connection\n",
    "                logger.info(\"Successfully connected to Kafka for topic management.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {i+1}/{max_retries} failed to connect to Kafka for topic check: {e}. Retrying...\")\n",
    "                time.sleep(5) # Wait before retrying\n",
    "        \n",
    "        if not admin_client:\n",
    "            raise ConnectionError(f\"Failed to connect to Kafka brokers after {max_retries} attempts.\")\n",
    "\n",
    "        topic_list = admin_client.list_topics()\n",
    "        if KAFKA_TOPIC not in topic_list:\n",
    "            logger.info(f\"Topic {KAFKA_TOPIC} does not exist. Creating...\")\n",
    "            # Adjust num_partitions and replication_factor as needed for your Kafka setup\n",
    "            new_topic = NewTopic(name=KAFKA_TOPIC, num_partitions=1, replication_factor=1)\n",
    "            admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "            logger.info(f\"Topic {KAFKA_TOPIC} created successfully.\")\n",
    "        else:\n",
    "            logger.info(f\"Topic {KAFKA_TOPIC} already exists.\")\n",
    "        admin_client.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"CRITICAL ERROR ensuring Kafka topic: {e}\")\n",
    "        # Depending on criticality, you might want to exit here\n",
    "        # raise # uncomment this if failing topic creation should stop the script\n",
    "\n",
    "try:\n",
    "    ensure_kafka_topic()\n",
    "except Exception as e:\n",
    "     logger.critical(f\"Could not ensure Kafka topic {KAFKA_TOPIC}. Exiting.\")\n",
    "     exit(1)\n",
    "\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Connecting to Kafka broker at {KAFKA_BROKER_URL}...\")\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[KAFKA_BROKER_URL],\n",
    "        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),\n",
    "        retries=5,\n",
    "        acks='all',\n",
    "        request_timeout_ms=30000 # Increase timeout for sending\n",
    "    )\n",
    "    producer.list_topics() # Force connection check\n",
    "    logger.info(\"Successfully connected to Kafka producer.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"CRITICAL: Error connecting to Kafka producer: {e}\")\n",
    "    exit(1) # Exit if producer cannot be initialized\n",
    "\n",
    "# --- Hàm gửi dữ liệu vào Kafka ---\n",
    "def on_send_success(record_metadata):\n",
    "    # logger.debug(f\"Successfully sent message to topic {record_metadata.topic} partition {record_metadata.partition} offset {record_metadata.offset}\")\n",
    "    pass # Log success only in debug for performance with large files\n",
    "\n",
    "def on_send_error(excp):\n",
    "    logger.error(f\"ERROR sending message to Kafka: {excp}\")\n",
    "\n",
    "def send_to_kafka_async(producer, topic, data, max_retries=3):\n",
    "    \"\"\"Sends data to Kafka asynchronously with retries.\"\"\"\n",
    "    if not data or not producer:\n",
    "        logger.warning(\"Attempted to send empty data or producer is not initialized.\")\n",
    "        return False\n",
    "    try:\n",
    "        # Use a future to check for errors later, or rely on errback\n",
    "        future = producer.send(topic, value=data)\n",
    "        future.add_callback(on_send_success)\n",
    "        future.add_errback(on_send_error)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        # This catches errors *before* sending (e.g., serialization issues)\n",
    "        logger.error(f\"Error preparing message for Kafka: {e}\")\n",
    "        # Note: The errback will handle transient send failures (network, etc.)\n",
    "        return False\n",
    "\n",
    "\n",
    "# --- Hàm xử lý giá trị NaN từ pandas ---\n",
    "def handle_nan(value):\n",
    "    \"\"\"Converts pandas NaN to None for JSON serialization.\"\"\"\n",
    "    import math # Import inside function to keep global scope cleaner\n",
    "\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    # Handle potential inf values if any exist (though unlikely in this dataset)\n",
    "    if isinstance(value, float) and (math.isinf(value) or math.isnan(value)):\n",
    "         return None\n",
    "    # Convert numpy types to standard Python types\n",
    "    if hasattr(value, 'item'): # Check if it's a numpy scalar\n",
    "         return value.item()\n",
    "    return value\n",
    "\n",
    "# --- Vòng lặp đọc CSV và gửi vào Kafka ---\n",
    "logger.info(f\"Attempting to read data from file: {CSV_FILE_PATH}\")\n",
    "\n",
    "processed_rows = 0\n",
    "successfully_prepared_sends = 0\n",
    "failed_prepares = 0\n",
    "rows_with_errors = 0 # Count rows that failed processing before sending\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        logger.critical(f\"Error: CSV file not found at {CSV_FILE_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Read the CSV file using pandas\n",
    "    # Using dtype=object for numerical columns can help inspect values before converting\n",
    "    # Or explicitly define dtypes if known and simple:\n",
    "    # dtypes = {\n",
    "    #     'player_id': 'Int64', # Use Int64 for nullable integer\n",
    "    #     'age': 'Float64', # Use Float64 for nullable float\n",
    "    #     # ... define for other columns with potential missing values\n",
    "    # }\n",
    "    # df = pd.read_csv(CSV_FILE_PATH, dtype=dtypes)\n",
    "\n",
    "    # Let pandas infer types, then handle NaNs manually during processing\n",
    "    df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "    logger.info(f\"Successfully read {len(df)} rows from {CSV_FILE_PATH}.\")\n",
    "\n",
    "    # Iterate over DataFrame rows\n",
    "    # Using itertuples() is often faster than iterrows()\n",
    "    # We convert tuples to dicts explicitly to control structure and NaN handling\n",
    "    for index, row in df.iterrows():\n",
    "        processed_rows += 1\n",
    "        try:\n",
    "            # Construct the data dictionary for Kafka message\n",
    "            player_data_record = {\n",
    "                'processing_timestamp': int(time.time() * 1000),\n",
    "                'player_id': handle_nan(row.get('player_id')),\n",
    "                'name': handle_nan(row.get('name')),\n",
    "                'player_club': handle_nan(row.get('player_club')),\n",
    "                'age': handle_nan(row.get('age')),\n",
    "                'position': handle_nan(row.get('position')),\n",
    "                'market_value': handle_nan(row.get('market_value')),\n",
    "                'nationality': handle_nan(row.get('nationality')),\n",
    "                'player_height': handle_nan(row.get('player_height')),\n",
    "                'player_agent': handle_nan(row.get('player_agent')),\n",
    "                'strong_foot': handle_nan(row.get('strong_foot')),\n",
    "                'contract_value_time': handle_nan(row.get('contract_value_time')),\n",
    "                'appearances': handle_nan(row.get('appearances')),\n",
    "                'PPG': handle_nan(row.get('PPG')),\n",
    "                'goals': handle_nan(row.get('goals')),\n",
    "                'assists': handle_nan(row.get('assists')),\n",
    "                'own_goals': handle_nan(row.get('own_goals')),\n",
    "                'substitutions_on': handle_nan(row.get('substitutions_on')),\n",
    "                'substitutions_off': handle_nan(row.get('substitutions_off')),\n",
    "                'yellow_cards': handle_nan(row.get('yellow_cards')),\n",
    "                'second_yellow_cards': handle_nan(row.get('second_yellow_cards')),\n",
    "                'red_cards': handle_nan(row.get('red_cards')),\n",
    "                'penalty_goals': handle_nan(row.get('penalty_goals')),\n",
    "                'minutes_per_goal': handle_nan(row.get('minutes_per_goal')),\n",
    "                'minutes_played': handle_nan(row.get('minutes_played')),\n",
    "                'source_file': CSV_FILE_PATH\n",
    "            }\n",
    "\n",
    "            # Basic validation: Skip rows with missing critical data like player_id or name\n",
    "            if player_data_record.get('player_id') is None or player_data_record.get('name') is None:\n",
    "                 logger.warning(f\"Skipping row {index} due to missing player_id or name.\")\n",
    "                 rows_with_errors += 1\n",
    "                 continue\n",
    "\n",
    "            # Send the record to Kafka\n",
    "            if send_to_kafka_async(producer, KAFKA_TOPIC, player_data_record):\n",
    "                successfully_prepared_sends += 1\n",
    "                # Log progress periodically\n",
    "                if processed_rows % 100 == 0: # Log every 100 rows\n",
    "                    logger.info(f\"Processed {processed_rows} rows so far. Prepared {successfully_prepared_sends} messages for sending.\")\n",
    "            else:\n",
    "                failed_prepares += 1\n",
    "                rows_with_errors += 1 # Count as an error if preparation failed\n",
    "\n",
    "        except Exception as row_e:\n",
    "            logger.error(f\"ERROR processing row {index}: {row_e}\")\n",
    "            rows_with_errors += 1\n",
    "\n",
    "        # Optional: Add a small delay between sending messages if needed\n",
    "        # time.sleep(0.01) # Small delay, adjust or remove based on performance needs\n",
    "\n",
    "except Exception as main_e:\n",
    "    logger.error(f\"An error occurred during file reading or processing: {main_e}\")\n",
    "\n",
    "finally:\n",
    "    logger.info(\"-\" * 30)\n",
    "    logger.info(\"CSV processing finished.\")\n",
    "    logger.info(f\"Total rows processed: {processed_rows}\")\n",
    "    logger.info(f\"Rows skipped or failed during preparation: {rows_with_errors}\")\n",
    "    logger.info(f\"Attempted to send {successfully_prepared_sends} messages to Kafka.\")\n",
    "\n",
    "    if producer:\n",
    "        logger.info(\"Flushing Kafka producer (waiting for pending messages)...\")\n",
    "        try:\n",
    "            # producer.flush() is blocking and waits for all async sends to complete\n",
    "            producer.flush(timeout=60) # Wait up to 60 seconds\n",
    "            logger.info(\"Kafka producer flushed.\")\n",
    "        except Exception as flush_e:\n",
    "            logger.error(f\"ERROR during producer flush: {flush_e}\")\n",
    "        finally:\n",
    "            logger.info(\"Closing Kafka producer.\")\n",
    "            producer.close()\n",
    "\n",
    "    logger.info(\"Script finished.\")\n",
    "\n",
    "# --- END OF FILE csv_to_kafka.py ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
