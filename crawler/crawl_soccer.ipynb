{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from kafka import KafkaProducer\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka.errors import KafkaError\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import signal\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# --- Cấu hình Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Cấu hình Kafka ---\n",
    "KAFKA_BROKER_URL = os.getenv('KAFKA_BROKER_URL', 'localhost:9092')\n",
    "KAFKA_TOPIC = 'football_players'\n",
    "\n",
    "# --- File paths ---\n",
    "PATH_TO_LEAGUES = \"Leagues.csv\"\n",
    "PATH_TO_CLUBS = \"Clubs.csv\"\n",
    "PATH_TO_PLAYERS_LINK = \"All_Players_Link.csv\"\n",
    "PATH_TO_FINAL_DATA = \"Final_Data.csv\"\n",
    "\n",
    "# --- HTTP session với retry logic ---\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# --- Headers cho requests ---\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "# --- Xử lý tín hiệu ngắt ---\n",
    "producer = None\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    logger.info(\"Received interrupt signal. Cleaning up...\")\n",
    "    if producer:\n",
    "        producer.flush(timeout=60)\n",
    "        producer.close()\n",
    "    logger.info(\"Cleanup completed. Exiting.\")\n",
    "    exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "signal.signal(signal.SIGTERM, signal_handler)\n",
    "\n",
    "# --- Kiểm tra và tạo Kafka topic ---\n",
    "def check_and_create_topic():\n",
    "    try:\n",
    "        admin_client = KafkaAdminClient(bootstrap_servers=[KAFKA_BROKER_URL])\n",
    "        topic_list = admin_client.list_topics()\n",
    "        if KAFKA_TOPIC not in topic_list:\n",
    "            logger.info(f\"Topic '{KAFKA_TOPIC}' does not exist. Creating...\")\n",
    "            new_topic = NewTopic(name=KAFKA_TOPIC, num_partitions=1, replication_factor=1)\n",
    "            admin_client.create_topics(new_topics=[new_topic], validate_only=False)\n",
    "            logger.info(f\"Topic '{KAFKA_TOPIC}' created successfully.\")\n",
    "        else:\n",
    "            logger.info(f\"Topic '{KAFKA_TOPIC}' already exists.\")\n",
    "        admin_client.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking/creating topic: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Hàm gửi dữ liệu vào Kafka (bất đồng bộ) ---\n",
    "def on_send_success(record_metadata):\n",
    "    pass\n",
    "\n",
    "def on_send_error(excp):\n",
    "    logger.error(f\"ERROR sending message to Kafka: {excp}\")\n",
    "\n",
    "def send_to_kafka_async(producer, topic, data, max_retries=3):\n",
    "    if not data or not producer:\n",
    "        return False\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            producer.send(topic, value=data).add_callback(on_send_success).add_errback(on_send_error)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Attempt {attempt+1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                logger.error(f\"Failed to send after {max_retries} attempts.\")\n",
    "                return False\n",
    "\n",
    "# --- Hàm nhập danh sách quốc gia ---\n",
    "def get_countries():\n",
    "    logger.info(\"Nhập danh sách quốc gia (ID và tên). Nhập 'done' khi hoàn tất.\")\n",
    "    countries = []\n",
    "    while True:\n",
    "        country_id = input(\"Nhập CountryID (hoặc 'done' để kết thúc): \")\n",
    "        if country_id.lower() == 'done':\n",
    "            break\n",
    "        country_name = input(\"Nhập tên quốc gia: \")\n",
    "        try:\n",
    "            countries.append({'CountryID': int(country_id), 'Country': country_name})\n",
    "        except ValueError:\n",
    "            logger.error(\"CountryID phải là số. Vui lòng nhập lại.\")\n",
    "    countries_df = pd.DataFrame(countries)\n",
    "    if countries_df.empty:\n",
    "        logger.warning(\"Không có quốc gia nào được nhập.\")\n",
    "    return countries_df\n",
    "\n",
    "# --- Hàm thu thập đường dẫn giải đấu ---\n",
    "def scrape_leagues(countries):\n",
    "    league_name, league_url, league_id = [], [], []\n",
    "    for i in range(len(countries)):\n",
    "        url = f\"https://www.transfermarkt.com/wettbewerbe/national/wettbewerbe/{countries.loc[i,'CountryID']}\"\n",
    "        logger.info(f\"Đang thu thập giải đấu từ {countries.loc[i,'Country']}...\")\n",
    "        try:\n",
    "            # time.sleep(2)\n",
    "            page = session.get(url, headers=headers, timeout=10)\n",
    "            page.raise_for_status()\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            \n",
    "            league_spans = soup.select('.inline-table a')[1:3]  # Lấy hai giải đấu hàng đầu\n",
    "            for span in league_spans:\n",
    "                href = span.get('href')\n",
    "                league_name.append(span.get('title'))\n",
    "                league_url.append('https://www.transfermarkt.com' + href + '/plus/?saison_id=')\n",
    "                league_id_match = re.search(r'/wettbewerb/([A-Z0-9]+)$', href)\n",
    "                league_id.append(league_id_match.group(1) if league_id_match else None)\n",
    "        except RequestException as e:\n",
    "            logger.error(f\"Lỗi khi thu thập giải đấu từ {countries.loc[i,'Country']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    leagues = pd.DataFrame({'league_name': league_name, 'league_url': league_url, 'league_id': league_id})\n",
    "    leagues.to_csv(PATH_TO_LEAGUES, index=False, encoding='utf-8-sig')\n",
    "    logger.info(f\"Đã lưu {len(leagues)} giải đấu vào {PATH_TO_LEAGUES}\")\n",
    "    return leagues\n",
    "\n",
    "# --- Hàm thu thập đường dẫn CLB ---\n",
    "def scrape_clubs(leagues):\n",
    "    club_name, club_url = [], []\n",
    "    for _, row in leagues.iterrows():\n",
    "        league = row['league_name']\n",
    "        base_url = row['league_url']\n",
    "        for season in range(2024, 2025):  # Chỉ lấy mùa 2024\n",
    "            url = base_url + str(season)\n",
    "            logger.info(f\"Đang thu thập CLB từ {league} mùa {season}...\")\n",
    "            try:\n",
    "                # time.sleep(2)\n",
    "                page = session.get(url, headers=headers, timeout=10)\n",
    "                page.raise_for_status()\n",
    "                soup = BeautifulSoup(page.content, 'html.parser')\n",
    "                \n",
    "                club_links = soup.select(\"#yw1 .no-border-links a:nth-child(1)\")\n",
    "                for link in club_links:\n",
    "                    club_name.append(link.get('title'))\n",
    "                    club_url.append('https://www.transfermarkt.com' + link.get('href'))\n",
    "            except RequestException as e:\n",
    "                logger.error(f\"Lỗi khi thu thập CLB từ {league}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    clubs = pd.DataFrame({'club_name': club_name, 'club_url': club_url})\n",
    "    clubs.to_csv(PATH_TO_CLUBS, index=False, encoding='utf-8-sig')\n",
    "    logger.info(f\"Đã lưu {len(clubs)} CLB vào {PATH_TO_CLUBS}\")\n",
    "    return clubs\n",
    "\n",
    "# --- Hàm thu thập đường dẫn cầu thủ từ CLB (không dùng phân trang) ---\n",
    "def scrape_players(clubs):\n",
    "    player_links = set()\n",
    "    for _, row in clubs.iterrows():\n",
    "        club_url = row['club_url']\n",
    "        club_name = row['club_name']\n",
    "        logger.info(f\"Đang thu thập cầu thủ từ {club_name}...\")\n",
    "        try:\n",
    "            # time.sleep(2)\n",
    "            response = session.get(club_url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            players_list = soup.select(\".inline-table .hauptlink > a\")\n",
    "            for p in players_list:\n",
    "                link = p.get('href')\n",
    "                if link:\n",
    "                    player_links.add(link)\n",
    "\n",
    "            logger.info(f\"Collected {len(player_links)} unique player links so far for {club_name}.\")\n",
    "        except RequestException as e:\n",
    "            logger.error(f\"Failed to crawl {club_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if player_links:\n",
    "        df_links = pd.DataFrame(list(player_links), columns=[\"0\"])\n",
    "        df_links.to_csv(PATH_TO_PLAYERS_LINK, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Saved {len(player_links)} unique player links to {PATH_TO_PLAYERS_LINK}\")\n",
    "    else:\n",
    "        logger.warning(\"No player links collected.\")\n",
    "\n",
    "    return list(player_links)\n",
    "\n",
    "# --- Hàm thu thập dữ liệu cầu thủ và thống kê ---\n",
    "def scrape_player_and_stats(url):\n",
    "    try:\n",
    "        pattern = r\"/(\\d+)$\"\n",
    "        match = re.search(pattern, url)\n",
    "        if not match:\n",
    "            logger.error(f\"Invalid URL format: {url}\")\n",
    "            return None\n",
    "        player_id = match.group(1)\n",
    "        data = {\"player_id\": player_id}\n",
    "\n",
    "        page = session.get(url, headers=headers, timeout=10)\n",
    "        page.raise_for_status()\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            data[\"name\"] = soup.select_one('h1[class=\"data-header__headline-wrapper\"]').text.split(\"\\n\")[-1].strip()\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract name for player ID {player_id}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            data[\"player_club\"] = soup.select_one(\"span[class='data-header__club']\").text.strip()\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract club for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            age_text = soup.select_one('li[class=\"data-header__label\"]').text.split(\"\\n\")[-2].split()[-1].strip(\"()\")\n",
    "            data[\"age\"] = float(age_text)\n",
    "        except (AttributeError, ValueError, IndexError):\n",
    "            logger.error(f\"Failed to extract age for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            data[\"position\"] = soup.find('dd', class_='detail-position__position').text.strip()\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract position for player {data['name']}\")\n",
    "            data[\"position\"] = None\n",
    "\n",
    "        if data[\"position\"] == \"Goalkeeper\":\n",
    "            logger.info(f\"Skipping player {data['name']} (ID: {player_id}) because they are a goalkeeper\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            market_value = soup.select_one('a[class=\"data-header__market-value-wrapper\"]').text.split(\" \")[0].replace('€', '')\n",
    "            if \"m\" in market_value:\n",
    "                data[\"market_value\"] = float(market_value.replace(\"m\", \"\")) * 1000\n",
    "            elif \"k\" in market_value:\n",
    "                data[\"market_value\"] = float(market_value.replace(\"k\", \"\"))\n",
    "            else:\n",
    "                data[\"market_value\"] = float(market_value)\n",
    "        except (AttributeError, ValueError, IndexError):\n",
    "            logger.error(f\"Failed to extract market value for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            data[\"nationality\"] = soup.find('span', itemprop=\"nationality\").text.strip()\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract nationality for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            height_text = re.search(r\"Height:.*?([0-9].*?)\\n\", soup.text, re.DOTALL).group(1).strip().split(\" \")[0].replace(\",\", \".\")\n",
    "            data[\"player_height\"] = float(height_text)\n",
    "        except (AttributeError, ValueError, IndexError):\n",
    "            logger.error(f\"Failed to extract height for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        # try:\n",
    "        #     data[\"player_agent\"] = re.search(r\"Agent:.*?([A-z].*?)\\n\", soup.text, re.DOTALL).group(1).strip()\n",
    "        # except (AttributeError, IndexError):\n",
    "        #     data[\"player_agent\"] = \"None\"\n",
    "        # except ValueError:\n",
    "        #     logger.error(f\"Failed to extract agent for player {data['name']}\")\n",
    "        #     return None\n",
    "\n",
    "        try:\n",
    "            name_span = soup.find_all('span', class_='info-table__content info-table__content--regular', string=lambda text: text and 'name' in text.lower())\n",
    "            data[\"strong_foot\"] = soup.select('span[class=\"info-table__content info-table__content--bold\"]')[6 if name_span else 5].text.strip()\n",
    "            if data[\"strong_foot\"] not in ['left', 'right', 'both']:\n",
    "                logger.error(f\"Invalid strong foot value for player {data['name']}: {data['strong_foot']}\")\n",
    "                return None\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract strong foot for player {data['name']}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            contract_text = re.search(r\"Contract expires: (.*)\", soup.text).group(1).split()[-1]\n",
    "            data[\"contract_value_time\"] = float(contract_text) if contract_text.isdigit() else contract_text\n",
    "        except (AttributeError, IndexError):\n",
    "            logger.error(f\"Failed to extract contract expiry for player {data['name']}\")\n",
    "            return None\n",
    "        except ValueError:\n",
    "            data[\"contract_value_time\"] = \"None\"\n",
    "\n",
    "        name_hyphenated = url.split('com/')[-1].split('/profil')[0].replace(' ', '-')\n",
    "        stat_url = f\"https://www.transfermarkt.com/{name_hyphenated}/leistungsdatendetails/spieler/{player_id}/plus/1?saison=2024&verein=&liga=&wettbewerb=&pos=&trainer_id=\"\n",
    "        # time.sleep(2)\n",
    "        stat_page = session.get(stat_url, headers=headers, timeout=10)\n",
    "        stat_page.raise_for_status()\n",
    "        stat_soup = BeautifulSoup(stat_page.content, \"html.parser\")\n",
    "\n",
    "        def extract_stat(index, default=0):\n",
    "            try:\n",
    "                value = stat_soup.find_all(\"td\", {\"class\": \"zentriert\"})[index].text\n",
    "                return float(value) if value != \"-\" else default\n",
    "            except (ValueError, AttributeError, IndexError):\n",
    "                return None\n",
    "\n",
    "        try:\n",
    "            minutes_per_goal = stat_soup.find_all(\"td\", {\"class\": \"rechts\"})[1].text.split(\"'\")[0]\n",
    "            minutes_per_goal = float(minutes_per_goal) if minutes_per_goal != \"-\" else 0\n",
    "        except (ValueError, AttributeError, IndexError):\n",
    "            minutes_per_goal = None\n",
    "\n",
    "        try:\n",
    "            minutes_played = stat_soup.find_all(\"td\", {\"class\": \"rechts\"})[2].text.split(\"'\")[0]\n",
    "            minutes_played = float(minutes_played) if minutes_played != \"-\" else 0\n",
    "        except (ValueError, AttributeError, IndexError):\n",
    "            minutes_played = None\n",
    "\n",
    "        data.update({\n",
    "            \"goalkeeper_or_not\": '0',\n",
    "            \"appearances\": extract_stat(1),\n",
    "            \"PPG\": extract_stat(2),\n",
    "            \"goals\": extract_stat(3),\n",
    "            \"assists\": extract_stat(4),\n",
    "            \"own_goals\": extract_stat(5),\n",
    "            \"substitutions_on\": extract_stat(6),\n",
    "            \"substitutions_off\": extract_stat(7),\n",
    "            \"yellow_cards\": extract_stat(8),\n",
    "            \"second_yellow_cards\": extract_stat(9),\n",
    "            \"red_cards\": extract_stat(10),\n",
    "            \"penalty_goals\": extract_stat(11),\n",
    "            \"minutes_per_goal\": minutes_per_goal,\n",
    "            \"minutes_played\": minutes_played,\n",
    "            \"goals_conceded\": 0.0,\n",
    "            \"clean_sheet\": 0.0\n",
    "        })\n",
    "\n",
    "        data['crawl_timestamp'] = int(time.time() * 1000)\n",
    "        # data['source'] = url\n",
    "        return data\n",
    "    except RequestException as e:\n",
    "        logger.error(f\"Request failed for URL {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error for URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Hàm chính ---\n",
    "def main():\n",
    "    # Nhập danh sách quốc gia\n",
    "    countries = get_countries()\n",
    "    if countries.empty:\n",
    "        logger.error(\"Không có quốc gia nào được nhập. Thoát chương trình.\")\n",
    "        return\n",
    "\n",
    "    # Thu thập đường dẫn giải đấu\n",
    "    leagues = scrape_leagues(countries)\n",
    "    if leagues.empty:\n",
    "        logger.error(\"Không thu thập được giải đấu nào. Thoát chương trình.\")\n",
    "        return\n",
    "\n",
    "    # Thu thập đường dẫn CLB\n",
    "    clubs = scrape_clubs(leagues)\n",
    "    if clubs.empty:\n",
    "        logger.error(\"Không thu thập được CLB nào. Thoát chương trình.\")\n",
    "        return\n",
    "\n",
    "    # Thu thập đường dẫn cầu thủ\n",
    "    player_links = scrape_players(clubs)\n",
    "    if not player_links:\n",
    "        logger.error(\"Không thu thập được liên kết cầu thủ nào. Thoát chương trình.\")\n",
    "        return\n",
    "\n",
    "    # Kiểm tra và tạo Kafka topic\n",
    "    check_and_create_topic()\n",
    "\n",
    "    # Khởi tạo Kafka producer\n",
    "    try:\n",
    "        logger.info(f\"Connecting to Kafka broker at {KAFKA_BROKER_URL}...\")\n",
    "        global producer\n",
    "        producer = KafkaProducer(\n",
    "            bootstrap_servers=[KAFKA_BROKER_URL],\n",
    "            value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),\n",
    "            retries=5,\n",
    "            acks='all'\n",
    "        )\n",
    "        logger.info(\"Successfully connected to Kafka.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"CRITICAL: Error connecting to Kafka: {e}\")\n",
    "        return\n",
    "\n",
    "    # Khởi tạo DataFrame cho backup\n",
    "    columns = [\n",
    "        \"player_id\", \"name\", \"player_club\", \"age\", \"position\", \"goalkeeper_or_not\",\n",
    "        \"market_value\", \"nationality\", \"player_height\", \"strong_foot\",\n",
    "        \"contract_value_time\", \"appearances\", \"PPG\", \"goals\", \"assists\", \"own_goals\",\n",
    "        \"substitutions_on\", \"substitutions_off\", \"yellow_cards\", \"second_yellow_cards\",\n",
    "        \"red_cards\", \"penalty_goals\", \"minutes_per_goal\", \"minutes_played\",\n",
    "        \"goals_conceded\", \"clean_sheet\", \"crawl_timestamp\"  #, \"source\"\n",
    "    ]\n",
    "    final_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Biến đếm số lần gửi Kafka\n",
    "    successful_sends_attempted = 0\n",
    "    failed_prepares = 0\n",
    "\n",
    "    try:\n",
    "        # Xử lý từng cầu thủ\n",
    "        for i, link in enumerate(player_links):\n",
    "            url = f\"https://www.transfermarkt.com{link}\"\n",
    "            logger.info(f\"Scraping player {i+1}/{len(player_links)}: {url}\")\n",
    "            player_data = scrape_player_and_stats(url)\n",
    "            if player_data is None:\n",
    "                logger.warning(f\"Skipping player at index {i} due to scraping error or goalkeeper\")\n",
    "                continue\n",
    "\n",
    "            # Gửi dữ liệu vào Kafka\n",
    "            if send_to_kafka_async(producer, KAFKA_TOPIC, player_data):\n",
    "                successful_sends_attempted += 1\n",
    "                logger.info(f\"Successfully sent data for player ID {player_data['player_id']} to Kafka\")\n",
    "            else:\n",
    "                failed_prepares += 1\n",
    "                logger.error(f\"Failed to send data for player ID {player_data['player_id']} to Kafka\")\n",
    "\n",
    "            # Thêm vào DataFrame\n",
    "            final_data = pd.concat([final_data, pd.DataFrame([player_data])], ignore_index=True)\n",
    "\n",
    "            # Lưu vào CSV sau mỗi 10 cầu thủ\n",
    "            if (i + 1) % 10 == 0:\n",
    "                final_data.to_csv(PATH_TO_FINAL_DATA, index=False, encoding='utf-8-sig')\n",
    "                producer.flush()\n",
    "                logger.info(f\"Saved {i+1} players to {PATH_TO_FINAL_DATA}\")\n",
    "\n",
    "            # time.sleep(2)\n",
    "\n",
    "    finally:\n",
    "        logger.info(\"-\" * 30)\n",
    "        logger.info(\"Crawl loop finished.\")\n",
    "        logger.info(f\"Total Kafka send attempts prepared: {successful_sends_attempted}\")\n",
    "        logger.info(f\"Total Kafka send preparation failures: {failed_prepares}\")\n",
    "\n",
    "        # Lưu dữ liệu lần cuối\n",
    "        final_data.to_csv(PATH_TO_FINAL_DATA, index=False, encoding='utf-8-sig')\n",
    "        logger.info(f\"Final data saved to {PATH_TO_FINAL_DATA}\")\n",
    "\n",
    "        # Flush và đóng producer\n",
    "        if producer:\n",
    "            logger.info(\"Flushing Kafka producer (waiting for pending messages)...\")\n",
    "            try:\n",
    "                producer.flush(timeout=60)\n",
    "                logger.info(\"Kafka producer flushed.\")\n",
    "            except Exception as flush_e:\n",
    "                logger.error(f\"ERROR during producer flush: {flush_e}\")\n",
    "            finally:\n",
    "                logger.info(\"Closing Kafka producer.\")\n",
    "                producer.close()\n",
    "\n",
    "        logger.info(\"Script finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # country = pd.DataFrame([\n",
    "    #     {'CountryID': 189, 'Country': 'England'},\n",
    "    #     {'CountryID': 40, 'Country': 'Germany'},\n",
    "    #     {'CountryID': 75, 'Country': 'Italy'},\n",
    "    #     {'CountryID': 50, 'Country': 'France'},\n",
    "    #     {'CountryID': 157, 'Country': 'Spain'}\n",
    "    # ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
